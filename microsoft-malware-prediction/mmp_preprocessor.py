from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import gc
import numpy as np
import pandas as pd

def preprocess(training_data, num_components):
    print(training_data.shape)
    assert training_data.shape[1] == 83
    print("Cleaning garbage....")
    gc.collect()
    
    target = 'HasDetections'
    target_index = 'MachineIdentifier'
    print("Reorganizing data")
    training_data.set_index(target_index)
    
    training_targets = training_data[target]
    
    training_data = training_data.drop(labels='HasDetections', axis=1)
    
    cat_columns = training_data.select_dtypes(['category']).columns
    training_data[cat_columns] = training_data[cat_columns].apply(lambda x: x.cat.codes)
    
    training_data = training_data.fillna(0)
    
    training_index = training_data.index
    print("Reducing dimensions")
    X_std = StandardScaler().fit_transform(training_data)
    sklearn_pca = PCA(n_components=num_components)
    pca_training_data = sklearn_pca.fit_transform(X_std)
    
    pca_training_data = pd.DataFrame(pca_training_data)
    pca_training_data['MachineIdentifier'] = training_index
    pca_training_data = pca_training_data.set_index('MachineIdentifier')
    
    return pca_training_data, training_targets